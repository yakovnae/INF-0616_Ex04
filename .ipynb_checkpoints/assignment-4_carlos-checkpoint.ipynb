{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF-616 - Tarefa 4 - TF-IDF, seleção de características e redução de dimensionalidade\n",
    "\n",
    "Professor: Jacques Wainer -- wainer@ic.unicamp.br  \n",
    "Monitor: Lucas David -- ra188972@students.ic.unicamp.br\n",
    "Aluno: Carlos Eduardo Fernandes\n",
    "\n",
    "Instituto de Computação - Unicamp  \n",
    "2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(9510)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algumas funções que podem ser úteis mais a frente:\n",
    "\n",
    "def describe(x, y, classes):\n",
    "    \"\"\"Descreve um conjunto de dados.\n",
    "    \n",
    "    :param x: np.ndarray, as amostras no conjunto.\n",
    "    :param y: np.ndarray, os rotulos associados as amostras `x`.\n",
    "    :param classes: list, uma lista com os nomes de cada classe. \n",
    "    \"\"\"\n",
    "    samples, features = (len(x), len(x[0])) if isinstance(x, list) else x.shape\n",
    "\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    print('  frequência das classes:',\n",
    "          *('    %s: %i' % e for e in zip(classes, counts)),\n",
    "          sep='\\n')\n",
    "\n",
    "def show_datasets(x, y, classes, title):\n",
    "    \"\"\"Encontra um \"embedding\" de um conjunto que alinha as direções\n",
    "       de maximiza separação das amostras com os eixos da base canônica,\n",
    "       permitindo uma melhor vizualização do conjunto.\n",
    "    \n",
    "    :param x: np.ndarray, as amostras do conjunto.\n",
    "    :param y: np.ndarray, os rotulos associados as amostras `x`.\n",
    "    :param title: str, o titulo do conjunto a ser exibido.\n",
    "    \"\"\"\n",
    "    x = TruncatedSVD(n_components=2).fit_transform(x)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for c in np.unique(y):\n",
    "        s = y == c\n",
    "        plt.scatter(x[s][:, 0], x[s][:, 1], label=classes[c])\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Matriz de Confusão',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    width = max(6, len(classes))\n",
    "    plt.figure(figsize=(width, width // 2))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Verdadeiro')\n",
    "    plt.xlabel('Predito')\n",
    "\n",
    "def train_and_report(grid, train, test, classes):\n",
    "    if not isinstance(grid, GridSearchCV):\n",
    "        raise ValueError('Você deve passar um objeto da classe `GridSearchCV` '\n",
    "                         'à função `train_and_report`. Ajuste seu modelo e sua '\n",
    "                         'chamada apropriadamente.')\n",
    "    grid.fit(*train)\n",
    "    print('Melhores parâmetros:', grid.best_params_)\n",
    "    print('Melhor pontuação sobre validação:', grid.best_score_)\n",
    "    \n",
    "    print('Resultados sobre a validação cruzada:')\n",
    "    properties = ['params',\n",
    "                  'mean_fit_time',\n",
    "                  'rank_test_score',\n",
    "                  'mean_test_score']\n",
    "    display(pd.DataFrame({p: grid.cv_results_[p] for p in properties})\n",
    "              .set_index('rank_test_score')\n",
    "              .sort_index())\n",
    "\n",
    "    print('Resultados sobre o teste:')\n",
    "    p = grid.predict(test[0])\n",
    "    print(metrics.classification_report(test[1], p))\n",
    "    plot_confusion_matrix(metrics.confusion_matrix(test[1], p), classes,\n",
    "                          normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução ao 20 News Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algumas amostras em 20 News Groups:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arly 70s. It was called a Bricklin. The doors were</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>who upgraded their SI clock oscillator have\\nshare</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bit sooner than i intended to be...\\n\\ni'm looking</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ass1.iastate.edu&gt;:\\n&gt; &gt; Anyone know about the Weite</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acs1.ttu.edu (Pack Rat) writes...\\n&gt;&gt;&gt;\"Clear cautio</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>article &lt;1qv87v$4j3@transfer.stratus.com&gt; cdt@sw.</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>because of mail-bouncing probs (Sean, Debra, and</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>93:29 \"Although SCSI is twice as fasst as ESDI,\\n&gt;&gt;</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>that it is a\\n&gt;\\thardware compression board that wo</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                article  \\\n",
       "0  arly 70s. It was called a Bricklin. The doors were     \n",
       "1   who upgraded their SI clock oscillator have\\nshare    \n",
       "2   bit sooner than i intended to be...\\n\\ni'm looking    \n",
       "3  ass1.iastate.edu>:\\n> > Anyone know about the Weite    \n",
       "4  acs1.ttu.edu (Pack Rat) writes...\\n>>>\"Clear cautio    \n",
       "5   article <1qv87v$4j3@transfer.stratus.com> cdt@sw.     \n",
       "6   because of mail-bouncing probs (Sean, Debra, and      \n",
       "7  93:29 \"Although SCSI is twice as fasst as ESDI,\\n>>    \n",
       "8                                                         \n",
       "9   that it is a\\n>\\thardware compression board that wo   \n",
       "\n",
       "                      label  \n",
       "0  rec.autos                 \n",
       "1  comp.sys.mac.hardware     \n",
       "2  comp.sys.mac.hardware     \n",
       "3  comp.graphics             \n",
       "4  sci.space                 \n",
       "5  talk.politics.guns        \n",
       "6  sci.med                   \n",
       "7  comp.sys.ibm.pc.hardware  \n",
       "8  comp.os.ms-windows.misc   \n",
       "9  comp.sys.mac.hardware     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto 20 News Groups treino:\n",
      "  frequência das classes:\n",
      "    alt.atheism: 480\n",
      "    comp.graphics: 584\n",
      "    comp.os.ms-windows.misc: 591\n",
      "    comp.sys.ibm.pc.hardware: 590\n",
      "    comp.sys.mac.hardware: 578\n",
      "    comp.windows.x: 593\n",
      "    misc.forsale: 585\n",
      "    rec.autos: 594\n",
      "    rec.motorcycles: 598\n",
      "    rec.sport.baseball: 597\n",
      "    rec.sport.hockey: 600\n",
      "    sci.crypt: 595\n",
      "    sci.electronics: 591\n",
      "    sci.med: 594\n",
      "    sci.space: 593\n",
      "    soc.religion.christian: 599\n",
      "    talk.politics.guns: 546\n",
      "    talk.politics.mideast: 564\n",
      "    talk.politics.misc: 465\n",
      "    talk.religion.misc: 377\n"
     ]
    }
   ],
   "source": [
    "# O codigo abaixo ira baixar e carregar o conjunto `20newsgroups`.\n",
    "# Leia sobre este conjunto aqui: http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\n",
    "\n",
    "train, test = (datasets.fetch_20newsgroups(subset=subset, shuffle=True)\n",
    "               for subset in ('train', 'test'))\n",
    "\n",
    "print('Algumas amostras em 20 News Groups:')\n",
    "samples_to_show = 10\n",
    "display(pd.DataFrame({'article': [a[320:370] for a in train.data[:samples_to_show]],\n",
    "                      'label': np.asarray(train.target_names)[train.target[:samples_to_show]]}))\n",
    "\n",
    "print('Conjunto 20 News Groups treino:')\n",
    "describe(train.data, train.target, train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vamos binarizar o problema:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.classes_ = train.target_names\n",
    "\n",
    "computing_classes, rec_classes = ([t for t in train.target_names if tag in t]\n",
    "                                   for tag in ('comp.', 'soc.religion.'))\n",
    "computing_codes, rec_codes = (le.transform(t)\n",
    "                              for t in (computing_classes, rec_classes))\n",
    "\n",
    "binary_set = []\n",
    "\n",
    "for (data, target) in ((train.data, train.target), (test.data, test.target)):\n",
    "    computing_samples = np.in1d(target, computing_codes)\n",
    "    rec_samples = np.in1d(target, rec_codes)\n",
    "    \n",
    "    x0 = [tx for tx, selected in zip(data, computing_samples) if selected]\n",
    "    x1 = [tx for tx, selected in zip(data, rec_samples) if selected]\n",
    "    \n",
    "    binary_set += [(x0 + x1, np.concatenate((np.zeros(len(x0)), np.ones(len(x1)))))]\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = binary_set\n",
    "target_names = ['computação', 'religião']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2936 artigos relacionadas à computação\n",
      "599 artigos relacionados à religião\n"
     ]
    }
   ],
   "source": [
    "print((y_train == 0).sum(), 'artigos relacionadas à computação')\n",
    "print((y_train == 1).sum(), 'artigos relacionados à religião')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processando texto com o TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Vetorize as amostras de texto em *20newgroups* em um conjunto de características utilizando o TF-IDF. Utilize as palavras de parada da lingua inglesa.**\n",
    "\n",
    "2. **Em seguida, remova todas as features que não possuam uma variância maior ou igual à $t = 1e^{-4}$.**\n",
    "\n",
    "3. **Finalmente, treine um classificador `LogisticRegression` buscando por valores de `C` no intervalo *(100, 10,000)***\n",
    "\n",
    "Dica: use o parâmetro `verbose=2` quando iniciar o grid-search. Isso lhe dará uma ideia do progresso da busca, caso ela tome muito tempo.\n",
    "\n",
    "Bônus: tente executar todas ações de uma única vez, utilizando a classe `Pipeline`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 41685)\t0.4308605912717007\n",
      "  (0, 33113)\t0.29444712733440515\n",
      "  (0, 20130)\t0.1541815588643371\n",
      "  (0, 48965)\t0.19340249952131344\n",
      "  (0, 20111)\t0.04550239335712303\n",
      "  (0, 45244)\t0.015190325809030684\n",
      "  (0, 21707)\t0.3131001625906709\n",
      "  (0, 36633)\t0.016060099163198994\n",
      "  (0, 28628)\t0.03459916457657828\n",
      "  (0, 30550)\t0.015229093143919398\n",
      "  (0, 1583)\t0.05390948697915374\n",
      "  (0, 18962)\t0.040874535084660386\n",
      "  (0, 51103)\t0.04277347080123019\n",
      "  (0, 35394)\t0.030275562876881924\n",
      "  (0, 38600)\t0.02926474709054755\n",
      "  (0, 25669)\t0.030016483387921865\n",
      "  (0, 1187)\t0.08363451228057725\n",
      "  (0, 7876)\t0.08687877244847537\n",
      "  (0, 3705)\t0.06065780643263185\n",
      "  (0, 45392)\t0.06002741199917117\n",
      "  (0, 10714)\t0.09100953418513213\n",
      "  (0, 45660)\t0.11643778160772504\n",
      "  (0, 20153)\t0.11643778160772504\n",
      "  (0, 21810)\t0.07955949492137462\n",
      "  (0, 22589)\t0.059186626021297074\n",
      "  :\t:\n",
      "  (0, 29383)\t0.03299638859435776\n",
      "  (0, 30671)\t0.05432797972253624\n",
      "  (0, 39006)\t0.05742345192260613\n",
      "  (0, 30626)\t0.05721677763503455\n",
      "  (0, 45661)\t0.12259692870408188\n",
      "  (0, 24823)\t0.07197965653219907\n",
      "  (0, 25631)\t0.06065780643263185\n",
      "  (0, 41690)\t0.2241355943955306\n",
      "  (0, 18813)\t0.1120677971977653\n",
      "  (0, 33094)\t0.0753016167457714\n",
      "  (0, 50694)\t0.09008862642769122\n",
      "  (0, 34172)\t0.1120677971977653\n",
      "  (0, 38589)\t0.1120677971977653\n",
      "  (0, 20709)\t0.1120677971977653\n",
      "  (0, 13172)\t0.1120677971977653\n",
      "  (0, 28885)\t0.10590865010140846\n",
      "  (0, 49518)\t0.08922037149873499\n",
      "  (0, 41667)\t0.1120677971977653\n",
      "  (0, 10917)\t0.1120677971977653\n",
      "  (0, 1510)\t0.1120677971977653\n",
      "  (0, 51056)\t0.09670124976065672\n",
      "  (0, 34291)\t0.1120677971977653\n",
      "  (0, 50880)\t0.10590865010140846\n",
      "  (0, 5318)\t0.1120677971977653\n",
      "  (0, 48787)\t0.05012867492631526\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index (72801) out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-20698f1697cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mreduce_x_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Carlos/Tools/anaconda3/lib/python3.6/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0missequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;31m# row is slice, col is sequence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                 \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m        \u001b[0;31m# [1:2,[1,2]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m                 \u001b[0msliced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Carlos/Tools/anaconda3/lib/python3.6/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36mextractor\u001b[0;34m(indices, N)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mmin_indx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmin_indx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Carlos/Tools/anaconda3/lib/python3.6/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36mcheck_bounds\u001b[0;34m(indices, N)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mmax_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_indx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index (%d) out of range'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmax_indx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mmin_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index (72801) out of range"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "vectors_x = vectorizer.fit_transform(x_train)\n",
    "selector = VarianceThreshold(threshold=1/10000)\n",
    "\n",
    "reduce_x_train = selector.fit_transform(vectors_x)\n",
    "\n",
    "idx = np.where(selector.variances_ > 1/10000)[0]\n",
    "\n",
    "vectors = vectorizer.fit_transform(x_test)\n",
    "reduce_x_test = vectors[:,idx]\n",
    "\n",
    "C = np.logspace(0, 10, 100)\n",
    "params = dict(C=C)\n",
    "model = LogisticRegression()\n",
    "grid = GridSearchCV(model, params, cv=5, verbose=2)\n",
    "\n",
    "#train_and_report(grid,vectorizer\n",
    "#                 train=(reduce_x_train, y_train),\n",
    "#                 test=(x_test, y_test),\n",
    "#                 classes=target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reporte a ROC AUC sobre o conjunto de teste.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = ...\n",
    "# print('ROC:', ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando o conjunto após a transformação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resgata o modelo com os parâmetros de melhor pontuação sobre as\n",
    "# dobras de validação, retreinado sobre todo o conjunto de treino.\n",
    "model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Você usou o pipeline no passo anterior?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_used = True  # altere aqui se necessário\n",
    "\n",
    "if pipeline_used:\n",
    "    transformers = model.steps[:2]\n",
    "else:\n",
    "    tfidf = None # substitua por seu transformador TF-IDF.\n",
    "    vt = None # substitua por seu seletor de features baseado em variância.\n",
    "    transformers = [\n",
    "        ('tfidfvectorizer', tfidf),\n",
    "        ('variancethreshold', vt),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Cria um novo pipeline com o TF-IDF e o seletor de características.\n",
    "transformer = Pipeline(transformers)\n",
    "t_train = transformer.transform(x_train)\n",
    "\n",
    "print('Número de características após transformação TF-IDF:',\n",
    "      transformer.named_steps['tfidfvectorizer'].transform(x_train).shape[1])\n",
    "print('Número de características após seleção de características:', t_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Estatísticas sobre a variância das características antes e após a seleção:')\n",
    "\n",
    "vt = transformer.named_steps['variancethreshold']\n",
    "chosen_features = vt.get_support()\n",
    "\n",
    "var_stats = pd.concat((pd.Series(vt.variances_).describe(),\n",
    "                       pd.Series(vt.variances_[chosen_features]).describe()),\n",
    "                      axis=1)\n",
    "var_stats.columns = ['original', 'selected']\n",
    "display(var_stats.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exibindo um *embedding* para o conjunto:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_datasets(t_train, y_train.astype(int),\n",
    "              target_names,\n",
    "              title='20 News Groups/train (comp,rec)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redução de dimensionalidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Utilize o `Non-Negative Matrix Factorization (NMF)` para reduzir a dimensionalidade do conjunto 20 news groups binarizado e vetorizado (mas não reduzido com a seleção de características) acima.**\n",
    "2. **Utilize um classificador baseado no algoritmo Random Forest, buscando pelo número de árvores no intervalo *(100, 500)*.**\n",
    "\n",
    "Dica: use um `GridSearchCV#n_jobs` pequeno aqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = ...\n",
    "# model = ...\n",
    "# grid = ...\n",
    "train_and_report(grid,\n",
    "                 train=(z_train, y_train),\n",
    "                 test=(z_test, y_test),\n",
    "                 classes=target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reporte a ROC AUC sobre o conjunto de teste:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('ROC AUC:', ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Qual dos dois conjuntos de algoritmos apresentou o melhor resultado?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verifique se combater o desbalanceamento das classes em treino ao ponderar as amostras pelo inverso das frequências de suas classes (utilizando o parâmetro `class_weight='balanced'`) melhora os resultados:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# params = ...\n",
    "# model = ...\n",
    "# grid = ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
